{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import queue\n",
    "import threading\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, Any, Iterator, Generator, List\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pika\n",
    "from pika.exceptions import AMQPConnectionError\n",
    "import orjson\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    batch_size: int = 100\n",
    "    max_buffer_size: int = 1000\n",
    "    flush_interval: float = 5.0  # seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder that handles datetime objects\"\"\"\n",
    "    \n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientJSONParser:\n",
    "    \"\"\"Generator-based JSON parser for streaming data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_json_stream(stream: Iterator[bytes]) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"\n",
    "        Parse JSON data from a stream using generators to avoid loading everything into memory.\n",
    "        \"\"\"\n",
    "        buffer = b\"\"\n",
    "        for chunk in stream:\n",
    "            if isinstance(chunk, str):\n",
    "                chunk = chunk.encode('utf-8')\n",
    "            \n",
    "            buffer += chunk\n",
    "            \n",
    "            # Try to parse complete JSON objects from buffer\n",
    "            while buffer:\n",
    "                try:\n",
    "                    # Simple approach: look for complete JSON objects separated by newlines\n",
    "                    decoded_buffer = buffer.decode('utf-8', errors='ignore')\n",
    "                    \n",
    "                    # Split by newlines and try to parse each line\n",
    "                    lines = decoded_buffer.split('\\n')\n",
    "                    processed_lines = 0\n",
    "                    \n",
    "                    for i, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            processed_lines += 1\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            obj = orjson.loads(line.encode())\n",
    "                            yield obj\n",
    "                            processed_lines += 1\n",
    "                        except orjson.JSONDecodeError:\n",
    "                            # This line might be incomplete, stop processing\n",
    "                            break\n",
    "                    \n",
    "                    # Update buffer with unprocessed lines\n",
    "                    if processed_lines == len(lines):\n",
    "                        buffer = b\"\"\n",
    "                    else:\n",
    "                        buffer = '\\n'.join(lines[processed_lines:]).encode()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"JSON parsing error: {e}\")\n",
    "                    buffer = b\"\"\n",
    "                    break\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_single_json(data: bytes) -> Dict[str, Any]:\n",
    "        \"\"\"Parse single JSON object with error handling\"\"\"\n",
    "        try:\n",
    "            return orjson.loads(data)\n",
    "        except orjson.JSONDecodeError as e:\n",
    "            logger.error(f\"Failed to parse JSON: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"Transforms and aggregates data using iterator patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, aggregation_window: int = 60):\n",
    "        self.aggregation_window = aggregation_window\n",
    "        self.aggregation_buffer = defaultdict(lambda: defaultdict(int))\n",
    "        self.last_flush_time = time.time()\n",
    "    \n",
    "    def transform_data(self, data_stream: Generator[Dict[str, Any], None, None]) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"\n",
    "        Transform and aggregate data using generator pattern.\n",
    "        Processes one record at a time to maintain constant memory.\n",
    "        \"\"\"\n",
    "        for record in data_stream:\n",
    "            try:\n",
    "                # Simple transformation example\n",
    "                transformed = self._transform_single_record(record)\n",
    "                \n",
    "                # Aggregate data\n",
    "                self._aggregate_record(transformed)\n",
    "                \n",
    "                # Yield individual transformed record\n",
    "                yield transformed\n",
    "                \n",
    "                # Check if we should flush aggregates\n",
    "                if self._should_flush_aggregates():\n",
    "                    yield from self._flush_aggregates()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error transforming record: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def _transform_single_record(self, record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Transform a single record - ensure all values are JSON serializable\"\"\"\n",
    "        transformed = record.copy()\n",
    "        \n",
    "        # Convert timestamp to ISO string if present\n",
    "        if 'timestamp' in transformed:\n",
    "            try:\n",
    "                # Handle both numeric timestamps and ISO format strings\n",
    "                ts = transformed['timestamp']\n",
    "                if isinstance(ts, (int, float)):\n",
    "                    # Convert numeric timestamp to ISO string\n",
    "                    transformed['processed_at'] = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\n",
    "                elif isinstance(ts, str):\n",
    "                    # Assume it's already in ISO format or can be parsed\n",
    "                    transformed['processed_at'] = ts\n",
    "                else:\n",
    "                    # Fallback to current time\n",
    "                    transformed['processed_at'] = datetime.now(timezone.utc).isoformat()\n",
    "            except (ValueError, TypeError, OSError) as e:\n",
    "                logger.warning(f\"Could not parse timestamp {transformed['timestamp']}: {e}\")\n",
    "                transformed['processed_at'] = datetime.now(timezone.utc).isoformat()\n",
    "        else:\n",
    "            # Add current timestamp if not present\n",
    "            transformed['processed_at'] = datetime.now(timezone.utc).isoformat()\n",
    "        \n",
    "        # Add processing metadata with string timestamps\n",
    "        transformed['_processed_ts'] = datetime.now(timezone.utc).isoformat()\n",
    "        transformed['_transform_version'] = '1.0'\n",
    "        \n",
    "        # Normalize value if present\n",
    "        if 'value' in transformed:\n",
    "            try:\n",
    "                transformed['normalized_value'] = float(transformed['value']) / 100.0\n",
    "            except (ValueError, TypeError):\n",
    "                transformed['normalized_value'] = 0.0\n",
    "        \n",
    "        # Ensure all values are JSON serializable\n",
    "        return self._ensure_json_serializable(transformed)\n",
    "    \n",
    "    def _ensure_json_serializable(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Ensure all values in the dictionary are JSON serializable\"\"\"\n",
    "        serializable_data = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (str, int, float, bool, type(None))):\n",
    "                serializable_data[key] = value\n",
    "            elif isinstance(value, (list, tuple)):\n",
    "                serializable_data[key] = [self._ensure_json_serializable_item(item) for item in value]\n",
    "            elif isinstance(value, dict):\n",
    "                serializable_data[key] = self._ensure_json_serializable(value)\n",
    "            else:\n",
    "                # Convert to string representation\n",
    "                serializable_data[key] = str(value)\n",
    "        return serializable_data\n",
    "    \n",
    "    def _ensure_json_serializable_item(self, item: Any) -> Any:\n",
    "        \"\"\"Ensure a single item is JSON serializable\"\"\"\n",
    "        if isinstance(item, (str, int, float, bool, type(None))):\n",
    "            return item\n",
    "        elif isinstance(item, (list, tuple)):\n",
    "            return [self._ensure_json_serializable_item(i) for i in item]\n",
    "        elif isinstance(item, dict):\n",
    "            return self._ensure_json_serializable(item)\n",
    "        else:\n",
    "            return str(item)\n",
    "    \n",
    "    def _aggregate_record(self, record: Dict[str, Any]):\n",
    "        \"\"\"Aggregate record data in memory-efficient way\"\"\"\n",
    "        current_time = time.time()\n",
    "        window_key = int(current_time // self.aggregation_window)\n",
    "        \n",
    "        # Example aggregation by type\n",
    "        record_type = record.get('type', 'unknown')\n",
    "        self.aggregation_buffer[window_key][record_type] += 1\n",
    "    \n",
    "    def _should_flush_aggregates(self) -> bool:\n",
    "        \"\"\"Check if aggregates should be flushed based on time\"\"\"\n",
    "        return time.time() - self.last_flush_time >= self.aggregation_window\n",
    "    \n",
    "    def _flush_aggregates(self) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Flush aggregated data and yield aggregation records\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_window = int(current_time // self.aggregation_window)\n",
    "        \n",
    "        # Remove old windows and yield aggregates\n",
    "        windows_to_remove = []\n",
    "        for window_key, aggregates in self.aggregation_buffer.items():\n",
    "            if window_key < current_window - 1:  # Keep current and previous window\n",
    "                yield {\n",
    "                    'type': 'aggregation',\n",
    "                    'window_start': window_key * self.aggregation_window,\n",
    "                    'window_end': (window_key + 1) * self.aggregation_window,\n",
    "                    'aggregates': dict(aggregates),\n",
    "                    'processed_at': datetime.fromtimestamp(current_time, tz=timezone.utc).isoformat(),\n",
    "                    'record_count': sum(aggregates.values()),\n",
    "                    'window_start_iso': datetime.fromtimestamp(\n",
    "                        window_key * self.aggregation_window, tz=timezone.utc\n",
    "                    ).isoformat(),\n",
    "                    'window_end_iso': datetime.fromtimestamp(\n",
    "                        (window_key + 1) * self.aggregation_window, tz=timezone.utc\n",
    "                    ).isoformat()\n",
    "                }\n",
    "                windows_to_remove.append(window_key)\n",
    "        \n",
    "        # Clean up flushed windows\n",
    "        for window_key in windows_to_remove:\n",
    "            del self.aggregation_buffer[window_key]\n",
    "        \n",
    "        self.last_flush_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Manages database operations with connection pooling and proper type handling\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Dict[str, Any]):\n",
    "        self.db_config = db_config\n",
    "        self.connection_pool = []\n",
    "        self.max_pool_size = 5\n",
    "    \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a database connection from pool or create new one\"\"\"\n",
    "        try:\n",
    "            if self.connection_pool:\n",
    "                return self.connection_pool.pop()\n",
    "            else:\n",
    "                return psycopg2.connect(**self.db_config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database connection error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def return_connection(self, conn):\n",
    "        \"\"\"Return connection to pool\"\"\"\n",
    "        if len(self.connection_pool) < self.max_pool_size and not conn.closed:\n",
    "            self.connection_pool.append(conn)\n",
    "        else:\n",
    "            try:\n",
    "                conn.close()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def initialize_schema(self):\n",
    "        \"\"\"Initialize database schema with proper types\"\"\"\n",
    "        conn = self.get_connection()\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                # Use TIMESTAMP WITH TIME ZONE for proper time handling\n",
    "                cursor.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS processed_data (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        data JSONB,\n",
    "                        processed_at TIMESTAMP WITH TIME ZONE,\n",
    "                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    "                    )\n",
    "                \"\"\")\n",
    "                cursor.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS data_aggregations (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        window_start TIMESTAMP WITH TIME ZONE,\n",
    "                        window_end TIMESTAMP WITH TIME ZONE,\n",
    "                        aggregates JSONB,\n",
    "                        record_count INTEGER,\n",
    "                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    "                    )\n",
    "                \"\"\")\n",
    "                conn.commit()\n",
    "                logger.info(\"Database schema initialized with proper types\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Schema initialization error: {e}\")\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            self.return_connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageQueueManager:\n",
    "    \"\"\"Manages message queue operations\"\"\"\n",
    "    \n",
    "    def __init__(self, mq_config: Dict[str, Any]):\n",
    "        self.mq_config = mq_config\n",
    "        self._connection = None\n",
    "        self._channel = None\n",
    "    \n",
    "    def ensure_connection(self):\n",
    "        \"\"\"Ensure message queue connection is established\"\"\"\n",
    "        if self._connection is None or self._connection.is_closed:\n",
    "            try:\n",
    "                self._connection = pika.BlockingConnection(\n",
    "                    pika.ConnectionParameters(**self.mq_config)\n",
    "                )\n",
    "                self._channel = self._connection.channel()\n",
    "                self._channel.queue_declare(queue='processed_data', durable=True)\n",
    "                logger.info(\"Message queue connection established\")\n",
    "            except AMQPConnectionError as e:\n",
    "                logger.error(f\"Message queue connection failed: {e}\")\n",
    "                raise\n",
    "    \n",
    "    def publish_message(self, message: Dict[str, Any]):\n",
    "        \"\"\"Publish message to queue\"\"\"\n",
    "        self.ensure_connection()\n",
    "        try:\n",
    "            # Use custom JSON encoder for serialization\n",
    "            message_body = json.dumps(message, cls=JSONEncoder, ensure_ascii=False)\n",
    "            self._channel.basic_publish(\n",
    "                exchange='',\n",
    "                routing_key='processed_data',\n",
    "                body=message_body.encode('utf-8'),\n",
    "                properties=pika.BasicProperties(\n",
    "                    delivery_mode=2,  # make message persistent\n",
    "                    content_type='application/json',\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Message publishing failed: {e}\")\n",
    "            # Reset connection for next attempt\n",
    "            self._connection = None\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputManager:\n",
    "    \"\"\"Manages output to database and message queue with proper type handling\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Dict[str, Any], mq_config: Dict[str, Any]):\n",
    "        self.db_manager = DatabaseManager(db_config)\n",
    "        self.mq_manager = MessageQueueManager(mq_config)\n",
    "        self.db_batch = []\n",
    "        self.mq_batch = []\n",
    "        self.json_encoder = JSONEncoder()\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize output systems\"\"\"\n",
    "        self.db_manager.initialize_schema()\n",
    "        self.mq_manager.ensure_connection()\n",
    "    \n",
    "    def add_to_batch(self, record: Dict[str, Any], batch_type: str):\n",
    "        \"\"\"Add record to appropriate batch\"\"\"\n",
    "        if batch_type == 'db':\n",
    "            self.db_batch.append(record)\n",
    "        else:\n",
    "            self.mq_batch.append(record)\n",
    "    \n",
    "    def flush_batch(self, batch_type: str, force: bool = False):\n",
    "        \"\"\"Flush batch to destination\"\"\"\n",
    "        if batch_type == 'db' and (force or len(self.db_batch) >= 100):\n",
    "            self._flush_to_db()\n",
    "        elif batch_type == 'mq' and (force or len(self.mq_batch) >= 50):\n",
    "            self._flush_to_mq()\n",
    "    \n",
    "    def _convert_to_db_types(self, record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert record fields to proper database types\"\"\"\n",
    "        processed_record = record.copy()\n",
    "        \n",
    "        # Convert processed_at to datetime if it's a string\n",
    "        if 'processed_at' in processed_record:\n",
    "            processed_at = processed_record['processed_at']\n",
    "            if isinstance(processed_at, str):\n",
    "                try:\n",
    "                    # Parse ISO format string to datetime\n",
    "                    if processed_at.endswith('Z'):\n",
    "                        processed_at = processed_at.replace('Z', '+00:00')\n",
    "                    processed_record['processed_at'] = datetime.fromisoformat(processed_at)\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    logger.warning(f\"Could not parse processed_at {processed_at}: {e}\")\n",
    "                    # Fallback to current time\n",
    "                    processed_record['processed_at'] = datetime.now(timezone.utc)\n",
    "            elif isinstance(processed_at, (int, float)):\n",
    "                # Convert numeric timestamp to datetime\n",
    "                processed_record['processed_at'] = datetime.fromtimestamp(processed_at, tz=timezone.utc)\n",
    "        \n",
    "        return processed_record\n",
    "    \n",
    "    def _serialize_for_db(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Serialize record for database JSONB storage\"\"\"\n",
    "        try:\n",
    "            # Use orjson for efficient serialization if available, fallback to json with custom encoder\n",
    "            return orjson.dumps(record).decode('utf-8')\n",
    "        except:\n",
    "            return json.dumps(record, cls=JSONEncoder, ensure_ascii=False)\n",
    "    \n",
    "    def _flush_to_db(self):\n",
    "        \"\"\"Flush batch to database with proper type conversion\"\"\"\n",
    "        if not self.db_batch:\n",
    "            return\n",
    "            \n",
    "        conn = self.db_manager.get_connection()\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                for record in self.db_batch:\n",
    "                    processed_record = self._convert_to_db_types(record)\n",
    "                    serialized_data = self._serialize_for_db(record)\n",
    "                    \n",
    "                    if processed_record.get('type') == 'aggregation':\n",
    "                        # Handle aggregation records\n",
    "                        cursor.execute(\"\"\"\n",
    "                            INSERT INTO data_aggregations \n",
    "                            (window_start, window_end, aggregates, record_count)\n",
    "                            VALUES (%s, %s, %s, %s)\n",
    "                        \"\"\", (\n",
    "                            datetime.fromtimestamp(processed_record['window_start'], tz=timezone.utc),\n",
    "                            datetime.fromtimestamp(processed_record['window_end'], tz=timezone.utc),\n",
    "                            serialized_data,  # Store the entire record as JSON\n",
    "                            processed_record['record_count']\n",
    "                        ))\n",
    "                    else:\n",
    "                        # Handle regular records\n",
    "                        cursor.execute(\n",
    "                            \"INSERT INTO processed_data (data, processed_at) VALUES (%s, %s)\",\n",
    "                            (serialized_data, processed_record.get('processed_at'))\n",
    "                        )\n",
    "                conn.commit()\n",
    "                logger.info(f\"Flushed {len(self.db_batch)} records to database\")\n",
    "                self.db_batch.clear()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error flushing to database: {e}\")\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            self.db_manager.return_connection(conn)\n",
    "    \n",
    "    def _flush_to_mq(self):\n",
    "        \"\"\"Flush batch to message queue\"\"\"\n",
    "        if not self.mq_batch:\n",
    "            return\n",
    "            \n",
    "        success_count = 0\n",
    "        for record in self.mq_batch:\n",
    "            try:\n",
    "                self.mq_manager.publish_message(record)\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to publish message: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if success_count > 0:\n",
    "            logger.info(f\"Flushed {success_count} records to message queue\")\n",
    "            # Only remove successfully published messages\n",
    "            self.mq_batch = self.mq_batch[success_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingPipeline:\n",
    "    \"\"\"Main pipeline that orchestrates the entire process\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.parser = MemoryEfficientJSONParser()\n",
    "        self.transformer = DataTransformer()\n",
    "        self.output_manager = OutputManager(\n",
    "            db_config={\n",
    "                'host': 'localhost',\n",
    "                'database': 'dataprocessing',\n",
    "                'user': 'user',\n",
    "                'password': 'pass',\n",
    "                'port': 5272\n",
    "            },\n",
    "            mq_config={\n",
    "                'host': 'localhost',\n",
    "                'port': 5672\n",
    "            }\n",
    "        )\n",
    "        self.input_queue = queue.Queue(maxsize=config.max_buffer_size)\n",
    "        self.running = False\n",
    "        self.processing_thread = None\n",
    "        self.output_thread = None\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start the pipeline\"\"\"\n",
    "        self.running = True\n",
    "        self.output_manager.initialize()\n",
    "        \n",
    "        # Start processing threads\n",
    "        self.processing_thread = threading.Thread(target=self._process_data, daemon=True)\n",
    "        self.output_thread = threading.Thread(target=self._handle_output, daemon=True)\n",
    "        \n",
    "        self.processing_thread.start()\n",
    "        self.output_thread.start()\n",
    "        \n",
    "        logger.info(\"Data processing pipeline started\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the pipeline gracefully\"\"\"\n",
    "        self.running = False\n",
    "        # Flush any remaining data\n",
    "        self.output_manager.flush_batch('db', force=True)\n",
    "        self.output_manager.flush_batch('mq', force=True)\n",
    "        \n",
    "        if self.processing_thread:\n",
    "            self.processing_thread.join(timeout=5.0)\n",
    "        if self.output_thread:\n",
    "            self.output_thread.join(timeout=5.0)\n",
    "        \n",
    "        logger.info(\"Data processing pipeline stopped\")\n",
    "    \n",
    "    def ingest_data(self, data: bytes):\n",
    "        \"\"\"Ingest data from webhook - non-blocking\"\"\"\n",
    "        try:\n",
    "            self.input_queue.put(data, block=False)\n",
    "            return True\n",
    "        except queue.Full:\n",
    "            logger.warning(\"Input queue full, dropping data\")\n",
    "            return False\n",
    "    \n",
    "    def _process_data(self):\n",
    "        \"\"\"Process data from input queue using generators\"\"\"\n",
    "        def input_generator():\n",
    "            while self.running:\n",
    "                try:\n",
    "                    data = self.input_queue.get(timeout=1.0)\n",
    "                    yield data\n",
    "                    self.input_queue.task_done()\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "        \n",
    "        try:\n",
    "            # Create processing pipeline using generators\n",
    "            raw_data_stream = input_generator()\n",
    "            parsed_stream = self.parser.parse_json_stream(raw_data_stream)\n",
    "            transformed_stream = self.transformer.transform_data(parsed_stream)\n",
    "            \n",
    "            # Process transformed data\n",
    "            for transformed_record in transformed_stream:\n",
    "                # Route to appropriate output\n",
    "                if transformed_record.get('type') == 'aggregation':\n",
    "                    self.output_manager.add_to_batch(transformed_record, 'db')\n",
    "                else:\n",
    "                    self.output_manager.add_to_batch(transformed_record, 'mq')\n",
    "                    self.output_manager.add_to_batch(transformed_record, 'db')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Processing thread error: {e}\")\n",
    "    \n",
    "    def _handle_output(self):\n",
    "        \"\"\"Handle output batching and flushing\"\"\"\n",
    "        last_flush = time.time()\n",
    "        \n",
    "        while self.running:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Periodic flushing\n",
    "            if current_time - last_flush >= self.config.flush_interval:\n",
    "                self.output_manager.flush_batch('db')\n",
    "                self.output_manager.flush_batch('mq')\n",
    "                last_flush = current_time\n",
    "            \n",
    "            time.sleep(0.1)  # Small sleep to prevent busy waiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous webhook handler\n",
    "class WebhookHandler:\n",
    "    \"\"\"Handles incoming webhook requests synchronously\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: DataProcessingPipeline):\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def handle_webhook(self, request_data: bytes) -> Dict[str, Any]:\n",
    "        \"\"\"Handle incoming webhook data synchronously\"\"\"\n",
    "        success = self.pipeline.ingest_data(request_data)\n",
    "        return {\n",
    "            \"status\": \"accepted\" if success else \"rejected\",\n",
    "            \"message\": \"Data queued for processing\" if success else \"System busy, try again later\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 12:16:22,487 - __main__ - INFO - Database schema initialized with proper types\n",
      "2025-10-06 12:16:22,491 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('127.0.0.1', 5672)\n",
      "2025-10-06 12:16:22,492 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=57, family=2, type=1, proto=6, laddr=('127.0.0.1', 37748), raddr=('127.0.0.1', 5672)>\n",
      "2025-10-06 12:16:22,497 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x7fbb6c49a660>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x7fbb6c49a660> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).\n",
      "2025-10-06 12:16:22,511 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x7fbb6c49a660> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>\n",
      "2025-10-06 12:16:22,513 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x7fbb6c49a660> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>\n",
      "2025-10-06 12:16:22,516 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x7fbb6c49a660> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>\n",
      "2025-10-06 12:16:22,518 - pika.adapters.blocking_connection - INFO - Created channel=1\n",
      "2025-10-06 12:16:22,537 - __main__ - INFO - Message queue connection established\n",
      "2025-10-06 12:16:22,549 - __main__ - INFO - Data processing pipeline started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1: {'status': 'accepted', 'message': 'Data queued for processing', 'timestamp': '2025-10-06T11:16:22.555315+00:00'}\n",
      "Data 2: {'status': 'accepted', 'message': 'Data queued for processing', 'timestamp': '2025-10-06T11:16:22.658628+00:00'}\n",
      "Data 3: {'status': 'accepted', 'message': 'Data queued for processing', 'timestamp': '2025-10-06T11:16:22.758973+00:00'}\n",
      "Data 4: {'status': 'accepted', 'message': 'Data queued for processing', 'timestamp': '2025-10-06T11:16:22.859191+00:00'}\n",
      "Pipeline running for 70 seconds to demonstrate aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 12:17:34,397 - __main__ - INFO - Flushed 4 records to database\n",
      "2025-10-06 12:17:34,409 - __main__ - INFO - Flushed 4 records to message queue\n",
      "2025-10-06 12:17:34,412 - __main__ - INFO - Data processing pipeline stopped\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    config = ProcessingConfig(\n",
    "        batch_size=100,\n",
    "        max_buffer_size=1000,\n",
    "        flush_interval=5.0\n",
    "    )\n",
    "    \n",
    "    pipeline = DataProcessingPipeline(config)\n",
    "    \n",
    "    try:\n",
    "        pipeline.start()\n",
    "        \n",
    "        # Simulate webhook data - SYNCHRONOUS calls\n",
    "        webhook_handler = WebhookHandler(pipeline)\n",
    "        \n",
    "        # Simulate incoming data with various timestamp formats\n",
    "        sample_data = [\n",
    "            b'{\"type\": \"event\", \"value\": 100, \"timestamp\": 1234567890}',\n",
    "            b'{\"type\": \"event\", \"value\": 200, \"timestamp\": \"2023-10-05T12:00:00Z\"}',\n",
    "            b'{\"type\": \"metric\", \"value\": 50}',\n",
    "            b'{\"type\": \"error\", \"message\": \"test error\", \"timestamp\": 1759619228}'\n",
    "        ]\n",
    "        \n",
    "        for i, data in enumerate(sample_data):\n",
    "            result = webhook_handler.handle_webhook(data)\n",
    "            print(f\"Data {i+1}: {result}\")\n",
    "            time.sleep(0.1)\n",
    "        _time = 70\n",
    "        # Keep running for a while to see aggregation\n",
    "        print(f\"Pipeline running for {_time} seconds to demonstrate aggregation...\")\n",
    "        time.sleep(_time)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down...\")\n",
    "    finally:\n",
    "        pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
